import os
import io
import numpy as np
import pandas as pd
import requests

from scipy.io import arff
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from imblearn.over_sampling import SMOTE

# ì„¤ì • ë³€ìˆ˜
GITHUB_RAW_BASE = 'https://raw.githubusercontent.com/klainfo/NASADefectDataset/master/CleanedData/MDP/D\'\'/'
SAVE_DIR = './data'
DATASET_FILES = [
    'CM1.arff', 'JM1.arff', 'KC1.arff', 'KC3.arff',
    'MC1.arff', 'MC2.arff', 'MW1.arff', 'PC1.arff', 'PC2.arff',
    'PC3.arff', 'PC4.arff', 'PC5.arff'
]
FULL_PATHS = [GITHUB_RAW_BASE + f for f in DATASET_FILES]

def preprocess_and_save_data(full_url, save_directory):
    file_name = os.path.basename(full_url) 
    dataset_name = file_name.replace('.arff', '')

    print(f"ğŸ”„ [{dataset_name}] ì²˜ë¦¬ ì¤‘...")

    try:
        response = requests.get(full_url)
        response.raise_for_status() 
        content_string = response.content.decode('utf-8')
        data_io_string = io.StringIO(content_string) 
        
        arff_data, meta = arff.loadarff(data_io_string)
        df = pd.DataFrame(arff_data)
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ ì˜¤ë¥˜: {dataset_name} íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨. URL: {full_url}, ì˜¤ë¥˜: {e}")
        return False
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {dataset_name} ARFF íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False
    
    try:
        if len(df) == 0:
            print(f"âš ï¸ ê²½ê³ : {dataset_name} ë°ì´í„°ì…‹ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            return False

        # 1. ì¤‘ë³µ ì œê±° (ë°ì´í„° ë¬´ê²°ì„± í™•ë³´)
        initial_count = len(df)
        df.drop_duplicates(inplace=True)
        final_count = len(df)
        if initial_count != final_count:
            print(f"   â„¹ï¸ ì¤‘ë³µ ë°ì´í„° {initial_count - final_count}ê°œ ì œê±°ë¨")

        # 2. íƒ€ê²Ÿ ì»¬ëŸ¼ ì‹ë³„
        target_col = None
        # 'Defective' ë˜ëŠ” 'label' ë“± íƒ€ê²Ÿ ë³€ìˆ˜ëª… í›„ë³´ ê²€ìƒ‰
        possible_targets = ['Defective', 'defective', 'label', 'class']
        for col in df.columns:
            if col in possible_targets:
                target_col = col
                break
        if target_col is None:
             target_col = df.columns[-1] # ëª» ì°¾ìœ¼ë©´ ë§ˆì§€ë§‰ ì»¬ëŸ¼ì„ íƒ€ê²Ÿìœ¼ë¡œ ê°€ì •

        # 3. íƒ€ê²Ÿ ë³€ìˆ˜ ì¸ì½”ë”© (False/True -> 0/1)
        # ë°”ì´íŠ¸ ë¬¸ìì—´ì¸ ê²½ìš° ë””ì½”ë”©
        if df[target_col].dtype == object and isinstance(df[target_col].iloc[0], bytes):
            df[target_col] = df[target_col].apply(lambda x: x.decode('utf-8'))
            
        le = LabelEncoder()
        y = pd.Series(le.fit_transform(df[target_col]), name='Defective_Encoded')
        
        # 4. ì…ë ¥ ë³€ìˆ˜(X) ë¶„ë¦¬: í•´ë‹¹ ë°ì´í„°ì…‹ì— ìˆëŠ” ì†ì„± ê·¸ëŒ€ë¡œ ì‚¬ìš© (ë…ë¦½ ì²˜ë¦¬)
        X = df.drop(columns=[target_col]).copy()
        
        # 5. ë‚´ë¶€ ê²°ì¸¡ì¹˜(NaN) ì²˜ë¦¬
        if X.isnull().values.any():
            print(f"   âš ï¸ ë‚´ë¶€ ê²°ì¸¡ì¹˜(NaN) ë°œê²¬. ê° ì»¬ëŸ¼ì˜ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì¹˜í•©ë‹ˆë‹¤.")
            X.fillna(X.mean(), inplace=True)
            # X.fillna(0, inplace=True)

        # 6. ë°ì´í„° ë¶„í•  (Stratified Split)
        if len(y) > 1: 
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, stratify=y, random_state=42
            )
        else:
            X_train, X_test, y_train, y_test = X, X, y, y

        # 6-1. Train ë°ì´í„° í´ë˜ìŠ¤ ë¹„ìœ¨ ì¡°ì • (SMOTE)
        # Train ë°ì´í„°ì— ëŒ€í•´ì„œë§Œ ì ìš© (TestëŠ” ì›ë³¸ ë¹„ìœ¨ ìœ ì§€)
        if len(y_train) > 0:
            # Check counts
            n_0 = (y_train == 0).sum()
            n_1 = (y_train == 1).sum()
            
            # ëª©í‘œ: n_0 : n_1 = 3 : 1 (Class 0ì´ Majorityì¼ ë•Œ)
            # Class 0ì´ Class 1ì˜ 3ë°°ë³´ë‹¤ ë§ìœ¼ë©´, SMOTEë¡œ Class 1ì„ ëŠ˜ë ¤ì„œ ë¹„ìœ¨ì„ ì¡°ì •
            if n_0 > n_1 * 3:
                 print(f"   â„¹ï¸ Train ë°ì´í„° ë¹„ìœ¨ ì¡°ì • (SMOTE 0:1=3:1): Class 1 í™•ëŒ€ ({n_1} => {int(n_0*0.3)})")
                 # sampling_strategy = 032 means minority = 0.3 * majority
                 smote = SMOTE(sampling_strategy=0.3, random_state=42)
                 try:
                     X_train, y_train = smote.fit_resample(X_train, y_train)
                 except Exception as e:
                     print(f"   âš ï¸ SMOTE ì ìš© ì‹¤íŒ¨ (ë°ì´í„° ë¶€ì¡± ë“±): {e}. ì›ë³¸ ë°ì´í„° ìœ ì§€.")
            else:
                 pass

        # 7. ìŠ¤ì¼€ì¼ë§ (DNNìš© - í‘œì¤€í™”)
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)
        X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {dataset_name} ì „ì²˜ë¦¬ ë¡œì§ ìˆ˜í–‰ ì¤‘ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False
        
    if not os.path.exists(save_directory):
        os.makedirs(save_directory)

    try:
        # 8. ì €ì¥
        # RF/GPìš© (ì›ë³¸ ìŠ¤ì¼€ì¼)
        pd.concat([X_train.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1).to_csv(os.path.join(save_directory, f'{dataset_name}_train_rf.csv'), index=False)
        pd.concat([X_test.reset_index(drop=True), y_test.reset_index(drop=True)], axis=1).to_csv(os.path.join(save_directory, f'{dataset_name}_test_rf.csv'), index=False)
        
        # DNNìš© (í‘œì¤€í™” ìŠ¤ì¼€ì¼)
        pd.concat([X_train_scaled_df.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1).to_csv(os.path.join(save_directory, f'{dataset_name}_train_pt.csv'), index=False)
        pd.concat([X_test_scaled_df.reset_index(drop=True), y_test.reset_index(drop=True)], axis=1).to_csv(os.path.join(save_directory, f'{dataset_name}_test_pt.csv'), index=False)

        print(f"âœ… {dataset_name} ì™„ë£Œ (ì†ì„± ìˆ˜: {X.shape[1]} / ë°ì´í„° í¬ê¸°: {X.shape[0]} / Train {len(X_train)} / Test {len(X_test)}).")
        return True

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {file_name} íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

def run_preprocessing_pipeline(full_paths, save_directory):
    print(f"ğŸ“‚ ì „ì²˜ë¦¬ ë°ì´í„°ë¥¼ '{save_directory}' í´ë”ì— ì €ì¥í•©ë‹ˆë‹¤.\n")
    print("--- ê° ë°ì´í„°ì…‹ ë…ë¦½ ì „ì²˜ë¦¬ ì‹œì‘ ---")
    
    success_count = 0
    for path in full_paths:
        if preprocess_and_save_data(path, save_directory):
            success_count += 1
            
    print(f"\nğŸ‰ ì´ {len(full_paths)}ê°œ ì¤‘ {success_count}ê°œ ë°ì´í„°ì…‹ ì²˜ë¦¬ ì™„ë£Œ.")

if __name__ == "__main__":
    run_preprocessing_pipeline(FULL_PATHS, SAVE_DIR)