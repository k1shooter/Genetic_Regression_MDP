# classifiers/chirps_full.py

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from classifiers.util import load_data
import warnings
import platform

# [ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸]
try:
    from mlxtend.preprocessing import TransactionEncoder
    from mlxtend.frequent_patterns import fpgrowth
except ImportError:
    print("âŒ mlxtend ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. 'pip install mlxtend'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    exit()

warnings.filterwarnings("ignore")

DATASET_NAMES = ['CM1', 'JM1', 'KC1']

class CHIRPSExplainerEnhanced:
    def __init__(self, model, X_train, y_train, num_classes):
        self.model = model
        self.X_train = X_train
        self.y_train = y_train
        self.num_classes = num_classes
        self.feature_names = X_train.columns.tolist()
        
    def _extract_paths(self, instance):
        """[Step 1] ê²½ë¡œ ì¶”ì¶œ (Fix: float32 í˜•ë³€í™˜ ì ìš©)"""
        paths = []
        # sklearn íŠ¸ë¦¬ í•¨ìˆ˜ì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ float32ë¡œ ë³€í™˜
        instance_array = instance.values.reshape(1, -1).astype(np.float32)
        
        # ëª¨ë¸ ì „ì²´ ì˜ˆì¸¡
        prediction = self.model.predict(instance_array)[0]
        
        for estimator in self.model.estimators_:
            # ê°œë³„ íŠ¸ë¦¬ ì˜ˆì¸¡ í™•ì¸ (ë‹¤ìˆ˜ê²° íˆ¬í‘œì— ê¸°ì—¬í•œ íŠ¸ë¦¬ë§Œ ì‚¬ìš©)
            if estimator.predict(instance_array)[0] != prediction:
                continue
                
            tree = estimator.tree_
            # ê²½ë¡œ ì¶”ì¶œ
            node_indicator = tree.decision_path(instance_array)
            node_index = node_indicator.indices[node_indicator.indptr[0]:node_indicator.indptr[1]]

            path_snippets = []
            for node_id in node_index:
                if tree.children_left[node_id] == -1: continue
                    
                feature_idx = tree.feature[node_id]
                threshold = tree.threshold[node_id]
                
                # ê°’ ë¹„êµëŠ” ì›ë³¸ ë°ì´í„° ì‚¬ìš©
                feature_val = instance.iloc[feature_idx]
                op = "<=" if feature_val <= threshold else ">"
                
                # ì¡°ê±´ ì €ì¥ (Feature_Idx, Operator, Threshold)
                snippet = (feature_idx, op, round(threshold, 2))
                path_snippets.append(snippet)
            
            paths.append(path_snippets)
            
        return paths

    def _mine_frequent_patterns(self, paths, min_support=0.1):
        """[Step 2] FP-Growth íŒ¨í„´ ë§ˆì´ë‹"""
        if not paths: return []

        te = TransactionEncoder()
        te_ary = te.fit(paths).transform(paths)
        df = pd.DataFrame(te_ary, columns=te.columns_)

        try:
            frequent = fpgrowth(df, min_support=min_support, use_colnames=True)
        except Exception:
            return []
        
        if frequent.empty: return []

        snippets = []
        for _, row in frequent.iterrows():
            pattern = list(row['itemsets']) 
            support = row['support']
            # ê¸¸ì´ê°€ ê¸´ íŒ¨í„´ì— ì•½ê°„ì˜ ê°€ì¤‘ì¹˜
            score = support * (len(pattern) ** 0.5) 
            snippets.append((pattern, score))

        snippets.sort(key=lambda x: x[1], reverse=True)
        return [s[0] for s in snippets]

    def _calculate_stability(self, rule, target_class):
        """[Step 3] Stability ë° ì»¤ë²„ë¦¬ì§€ ê³„ì‚°"""
        if not rule: return 0.0, 0, 0
            
        mask = np.ones(len(self.X_train), dtype=bool)
        
        for feature_idx, op, threshold in rule:
            col_name = self.feature_names[feature_idx]
            values = self.X_train[col_name]
            if op == "<=": mask &= (values <= threshold)
            else: mask &= (values > threshold)
        
        covered_indices = np.where(mask)[0]
        n_covered = len(covered_indices)
        if n_covered == 0: return 0.0, 0, 0
            
        n_target = np.sum(self.y_train.iloc[covered_indices] == target_class)
        n_others = n_covered - n_target
        
        # ë…¼ë¬¸ì˜ Stability ê³µì‹
        stability = (n_target + 1) / (n_covered + self.num_classes)
        
        return stability, n_target, n_others

    def explain_instance(self, instance):
        """[Main] ì„¤ëª… ìƒì„±"""
        paths = self._extract_paths(instance)
        if not paths: return None
            
        ranked_patterns = self._mine_frequent_patterns(paths, min_support=0.1)
        
        # ì˜ˆì¸¡ê°’ í™•ì¸
        instance_array = instance.values.reshape(1, -1).astype(np.float32)
        target_class = self.model.predict(instance_array)[0]
        
        current_rule = []
        
        # ì „ì²´ ë°ì´í„° ê¸°ì¤€ ì´ˆê¸° Stability
        base_target = np.sum(self.y_train == target_class)
        best_stability = (base_target + 1) / (len(self.X_train) + self.num_classes)
        
        # Greedy Merging
        for pattern in ranked_patterns:
            candidate_rule = list(set(current_rule + pattern))
            stability, _, _ = self._calculate_stability(candidate_rule, target_class)
            
            if stability > best_stability:
                current_rule = candidate_rule
                best_stability = stability
        
        # Pruning
        final_rule = []
        if current_rule:
            for i in range(len(current_rule)):
                temp_rule = current_rule[:i] + current_rule[i+1:]
                stab, _, _ = self._calculate_stability(temp_rule, target_class)
                
                if stab < best_stability * 0.99:
                    final_rule.append(current_rule[i])
                else:
                    best_stability = stab
        
        if not final_rule: final_rule = current_rule

        final_stab, n_target, n_others = self._calculate_stability(final_rule, target_class)
        
        return {
            "rule": final_rule,
            "stability": final_stab,
            "n_target": n_target,
            "n_others": n_others,
            "target_class": target_class
        }

    def rule_to_string(self, rule):
        if not rule: return "No rule found."
        clauses = [f"({self.feature_names[f]} {o} {t})" for f, o, t in rule]
        return " AND ".join(clauses)

def save_rule_plot(exp, instance_id, dataset_name, save_dir):
    """[ì‹œê°í™”] ê·œì¹™ì˜ Stabilityì™€ Coverageë¥¼ ì‹œê°í™”"""
    plt.figure(figsize=(8, 5))
    
    categories = ['Target Class', 'Other Classes']
    counts = [exp['n_target'], exp['n_others']]
    colors = ['#1f77b4', '#d62728'] # íŒŒë‘(ì„±ê³µ), ë¹¨ê°•(ì‹¤íŒ¨)
    
    bars = plt.bar(categories, counts, color=colors, alpha=0.7, width=0.5)
    
    # ìˆ˜ì¹˜ í‘œì‹œ
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height,
                 f'{int(height)}', ha='center', va='bottom')
    
    plt.title(f"[CHIRPS] Rule Analysis for Instance {instance_id}\n(Stability: {exp['stability']:.3f})")
    plt.ylabel("Number of Samples Covered by Rule")
    plt.grid(axis='y', linestyle='--', alpha=0.3)
    
    # í…ìŠ¤íŠ¸ë¡œ ê·œì¹™ í‘œì‹œ (ê·¸ë˜í”„ í•˜ë‹¨ì—)
    plt.figtext(0.5, -0.1, f"Rule: {exp['readable_rule']}", ha="center", fontsize=9, 
                bbox={"facecolor":"orange", "alpha":0.1, "pad":5}, wrap=True)
    
    plt.tight_layout()
    filename = f"{dataset_name}_Instance_{instance_id}_Rule.png"
    plt.savefig(os.path.join(save_dir, filename), bbox_inches='tight')
    plt.close()

def run_analysis(dataset_name):
    print(f"\nğŸš€ Analyzing {dataset_name} with CHIRPS (Full Pipeline)...")
    X_train, y_train, X_test, y_test = load_data(dataset_name, data_type='rf')
    if X_train is None: return

    model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    model.fit(X_train, y_train)
    
    num_classes = len(np.unique(y_train))
    explainer = CHIRPSExplainerEnhanced(model, X_train, y_train, num_classes)
    
    # ì €ì¥ ê²½ë¡œ ì„¤ì •
    save_dir = f"analysis_results/CHIRPS_Full/{dataset_name}"
    os.makedirs(save_dir, exist_ok=True)
    
    # Defective(1)ì¸ ì¼€ì´ìŠ¤ ì¤‘ì—ì„œ 3ê°œë§Œ ìƒ˜í”Œë§
    target_indices = np.where(y_test == 1)[0]
    if len(target_indices) == 0:
        target_indices = range(3) # ì—†ìœ¼ë©´ ì•ìª½ 3ê°œ
    else:
        target_indices = target_indices[:3]
    
    results_list = []
    
    for i in target_indices:
        instance = X_test.iloc[i] 
        exp = explainer.explain_instance(instance)
        
        if exp:
            rule_str = explainer.rule_to_string(exp['rule'])
            exp['readable_rule'] = rule_str # ì‹œê°í™”ìš© ì¶”ê°€ ì €ì¥
            
            print(f"\n[Test ID {i}] Class: {exp['target_class']}, Stability: {exp['stability']:.3f}")
            print(f"  - Rule: {rule_str}")
            
            # ì‹œê°í™” ì €ì¥
            save_rule_plot(exp, i, dataset_name, save_dir)
            
            results_list.append({
                'Dataset': dataset_name,
                'Instance_ID': i,
                'Predicted_Class': exp['target_class'],
                'Stability': exp['stability'],
                'Covered_Target': exp['n_target'],
                'Covered_Others': exp['n_others'],
                'Rule': rule_str
            })
        else:
            print(f"\n[Test ID {i}] No rule found.")

    # ê²°ê³¼ CSV ì €ì¥
    if results_list:
        df = pd.DataFrame(results_list)
        csv_path = os.path.join(save_dir, f"{dataset_name}_chirps_rules.csv")
        df.to_csv(csv_path, index=False)
        print(f"ğŸ’¾ Results saved to: {csv_path}")

if __name__ == "__main__":
    for name in DATASET_NAMES:
        try:
            run_analysis(name)
        except Exception as e:
            print(f"Error {name}: {e}")
            import traceback
            traceback.print_exc()