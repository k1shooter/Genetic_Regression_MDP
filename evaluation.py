import os
import sys
import glob
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tabulate import tabulate
from datetime import datetime

def find_latest_result(pattern):
    """ì£¼ì–´ì§„ íŒ¨í„´ì— ë§ëŠ” ê°€ì¥ ìµœì‹  CSV íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤."""
    files = glob.glob(pattern)
    if not files: return None
    return max(files, key=os.path.getctime)

def standardize_columns(df):
    """ë‹¤ì–‘í•œ ì´ë¦„ì˜ ì»¬ëŸ¼ì„ í‘œì¤€ ì´ë¦„(Acc, F1, MCC)ìœ¼ë¡œ ë³€ê²½í•©ë‹ˆë‹¤."""
    if df.empty: return df
    
    # ê³µë°± ì œê±°
    df.columns = df.columns.str.strip()
    
    rename_map = {
        # Accuracy
        'Accuracy': 'Acc', 'Test Acc': 'Acc', 'Test_Acc': 'Acc',
        'DNN_Acc': 'Acc', 'RF_Acc': 'Acc', 'DNN_Accuracy': 'Acc', 'RF_Accuracy': 'Acc',
        
        # F1 Score
        'F1_Score': 'F1', 'F1_Defective': 'F1', 'Test F1': 'F1', 'Test_F1': 'F1',
        'DNN_F1': 'F1', 'RF_F1': 'F1', 'DNN_F1_Score': 'F1', 'RF_F1_Score': 'F1',
        
        # MCC
        'Test MCC': 'MCC', 'Test_MCC': 'MCC', 
        'DNN_MCC': 'MCC', 'RF_MCC': 'MCC',
        'MCC Score': 'MCC',
        
        # Others
        'Complexity': 'Cplx'
    }
    
    # rename ì ìš©
    df = df.rename(columns=rename_map)
    
    # í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ 0.0ìœ¼ë¡œ ì´ˆê¸°í™”
    for col in ['Acc', 'F1', 'MCC']:
        if col not in df.columns:
            df[col] = 0.0
            
    return df

def load_results():
    """ê° ëª¨ë¸ì˜ ìµœì‹  ê²°ê³¼ íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ í†µí•©í•©ë‹ˆë‹¤."""
    print("ğŸ“‚ ìµœì‹  ê²°ê³¼ íŒŒì¼ ë¡œë”© ì¤‘...")
    
    dfs = []

    # 1. DNN (Tuned)
    dnn_file = find_latest_result("optuna_dnn_results*.csv")
    if dnn_file:
        dnn_df = pd.read_csv(dnn_file)
        dnn_df = standardize_columns(dnn_df)
        dnn_df['Model'] = 'DNN (Tuned)'
        dfs.append(dnn_df)
    else:
        print("âš ï¸ DNN ê²°ê³¼ íŒŒì¼ ì—†ìŒ")

    # 2. Random Forest (Tuned)
    rf_file = find_latest_result("optuna_rf_results*.csv")
    if rf_file:
        rf_df = pd.read_csv(rf_file)
        rf_df = standardize_columns(rf_df)
        rf_df['Model'] = 'RF (Tuned)'
        dfs.append(rf_df)
    else:
        print("âš ï¸ RF ê²°ê³¼ íŒŒì¼ ì—†ìŒ")

    # 3. Naive Bayes
    nb_file = find_latest_result("naive_bayes_results_*.csv")
    if nb_file:
        nb_df = pd.read_csv(nb_file)
        nb_df = standardize_columns(nb_df)
        nb_df['Model'] = 'Naive Bayes'
        dfs.append(nb_df)
    else:
        print("âš ï¸ Naive Bayes ê²°ê³¼ íŒŒì¼ ì—†ìŒ")

    # 4. GP (GA-MO)
    gp_file = find_latest_result("ga_mo_results_*.csv")
    if gp_file:
        gp_raw = pd.read_csv(gp_file)
        gp_raw = standardize_columns(gp_raw)
        
        # Targetì´ ìˆë‹¤ë©´ MCC ìµœì í™” ê²°ê³¼ ìš°ì„ , ì—†ìœ¼ë©´ MCC ì ìˆ˜ ë†’ì€ ìˆœ
        if 'Target' in gp_raw.columns:
            mcc_target = gp_raw[gp_raw['Target'].str.upper() == 'MCC']
            if not mcc_target.empty:
                # ë°ì´í„°ì…‹ë³„ ìµœê³  ì„±ëŠ¥ 1ê°œë§Œ ì¶”ì¶œ (ëª¨ë¸ ëŒ€í‘œê°’)
                gp_best = mcc_target.sort_values(['Dataset', 'MCC'], ascending=[True, False]).drop_duplicates('Dataset')
            else:
                gp_best = gp_raw.sort_values(['Dataset', 'MCC'], ascending=[True, False]).drop_duplicates('Dataset')
        else:
            gp_best = gp_raw.sort_values(['Dataset', 'MCC'], ascending=[True, False]).drop_duplicates('Dataset')
            
        gp_df = gp_best.copy()
        gp_df['Model'] = 'GP (Ours)'
        dfs.append(gp_df)
    else:
        print("âš ï¸ GP ê²°ê³¼ íŒŒì¼ ì—†ìŒ")

    if not dfs:
        return pd.DataFrame()

    # í†µí•©
    all_df = pd.concat(dfs, ignore_index=True)
    
    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ë‚¨ê¸°ê³ , NaNì€ 0ìœ¼ë¡œ ì±„ì›€
    cols = ['Dataset', 'Model', 'Acc', 'F1', 'MCC', 'Cplx', 'Formula']
    for col in cols:
        if col not in all_df.columns:
            all_df[col] = pd.NA
            
    # ìˆ«ìí˜• ì»¬ëŸ¼ ê²°ì¸¡ì¹˜ 0 ì²˜ë¦¬
    num_cols = ['Acc', 'F1', 'MCC']
    all_df[num_cols] = all_df[num_cols].fillna(0.0)
    
    return all_df

def plot_comparison(df):
    """ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ ê·¸ë˜í”„ ìƒì„± ë° ì €ì¥"""
    if df.empty: return

    save_dir = "comparison_plots"
    os.makedirs(save_dir, exist_ok=True)
    
    metrics = ['F1', 'MCC']
    
    for metric in metrics:
        if metric not in df.columns: continue
        
        plt.figure(figsize=(12, 6))
        plot_data = df[df[metric] != 0] 
        
        if plot_data.empty: continue

        sns.barplot(data=plot_data, x='Dataset', y=metric, hue='Model', palette='viridis')
        plt.title(f'Model Comparison - {metric} Score', fontsize=15)
        plt.ylabel(metric)
        plt.ylim(-0.1, 1.1) 
        plt.grid(axis='y', linestyle='--', alpha=0.7)
        plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left')
        plt.tight_layout()
        
        filename = f"{save_dir}/comparison_{metric}.png"
        plt.savefig(filename)
        plt.close()
        print(f"ğŸ“Š {metric} ë¹„êµ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {filename}")

def get_best_models_df(df):
    """ê° ë°ì´í„°ì…‹ë³„ MCC, F1 ìµœê³  ëª¨ë¸ì„ ì°¾ì•„ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€"""
    # Datasetë³„ë¡œ ê·¸ë£¹í™”
    grouped = df.groupby('Dataset')
    
    best_mcc_map = {}
    best_f1_map = {}
    
    for name, group in grouped:
        # MCC Best (ë™ì ì í¬í•¨)
        max_mcc = group['MCC'].max()
        winners_mcc = group[group['MCC'] == max_mcc]['Model'].tolist()
        best_mcc_map[name] = ", ".join(winners_mcc)
        
        # F1 Best (ë™ì ì í¬í•¨)
        max_f1 = group['F1'].max()
        winners_f1 = group[group['F1'] == max_f1]['Model'].tolist()
        best_f1_map[name] = ", ".join(winners_f1)
        
    # ì›ë³¸ dfì— ë§¤í•‘ (1ë“± ì •ë³´ ì¶”ê°€)
    df['Best Model (MCC)'] = df['Dataset'].map(best_mcc_map)
    df['Best Model (F1)'] = df['Dataset'].map(best_f1_map)
    
    return df

def print_summary(df):
    """ìµœì¢… ìš”ì•½ í…Œì´ë¸” ì¶œë ¥ ë° ì €ì¥"""
    if df.empty: return

    print("\n" + "="*100)
    print("ğŸ† Final Performance Summary (All Models with Winner Info)")
    print("="*100)
    
    # Best Model ì •ë³´ ì¶”ê°€
    df = get_best_models_df(df)
    
    # ì¶œë ¥ìš© ì»¬ëŸ¼ ìˆœì„œ ì§€ì •
    display_cols = ['Dataset', 'Model', 'Acc', 'F1', 'MCC', 'Best Model (MCC)', 'Best Model (F1)']
    
    # ì •ë ¬: Dataset ì´ë¦„ìˆœ -> MCC ë‚´ë¦¼ì°¨ìˆœ
    df_sorted = df.sort_values(by=['Dataset', 'MCC'], ascending=[True, False])
    
    # ë°ì´í„° í¬ë§·íŒ…
    table_data = []
    for _, row in df_sorted.iterrows():
        table_data.append([
            row['Dataset'],
            row['Model'],
            f"{float(row['Acc']):.4f}",
            f"{float(row['F1']):.4f}",
            f"{float(row['MCC']):.4f}",
            row['Best Model (MCC)'],
            row['Best Model (F1)']
        ])
        
    # í™”ë©´ ì¶œë ¥
    print(tabulate(table_data, headers=display_cols, tablefmt="fancy_grid"))
    
    # CSV ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    # ì €ì¥í•  ë•ŒëŠ” ìˆ˜ì‹ ì •ë³´ë„ í¬í•¨
    save_cols = display_cols + ['Cplx', 'Formula']
    valid_cols = [c for c in save_cols if c in df.columns]
    
    filename = f"final_evaluation_summary_{timestamp}.csv"
    df_sorted[valid_cols].to_csv(filename, index=False)
    print(f"\nğŸ’¾ ì „ì²´ ìƒì„¸ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filename}")

    compare_formulas(df)

def compare_formulas(df):
    print("\n" + "="*80)
    print("ğŸ” Interpretability Comparison: GP vs RF (Simple Tree)")
    print("="*80)
    
    # GP ìˆ˜ì‹
    gp_df = df[df['Model'] == 'GP (Ours)']
    
    # RF ìˆ˜ì‹ (ë³„ë„ íŒŒì¼ì—ì„œ ë¡œë“œ)
    rf_file = find_latest_result("random_forest_formulas_*.csv")
    rf_df = pd.DataFrame()
    if rf_file:
        try:
            rf_raw = pd.read_csv(rf_file)
            if not rf_raw.empty:
                rf_df = rf_raw[rf_raw['Tree_Index'] == 0][['Dataset', 'Formula']].rename(columns={'Formula': 'RF_Formula'})
        except: pass

    # ë°ì´í„°ì…‹ ë¦¬ìŠ¤íŠ¸
    datasets = sorted(df['Dataset'].unique())
    
    for ds in datasets:
        print(f"\nğŸ“Œ Dataset: {ds}")
        
        # GP Formula ì¶œë ¥
        gp_row = gp_df[gp_df['Dataset'] == ds]
        if not gp_row.empty and pd.notna(gp_row.iloc[0]['Formula']):
            cplx = gp_row.iloc[0]['Cplx']
            form = gp_row.iloc[0]['Formula']
            print(f"   [GP] (Cplx: {cplx}): {form}")
        else:
            print("   [GP] -")
            
        # RF Formula ì¶œë ¥
        if not rf_df.empty:
            rf_row = rf_df[rf_df['Dataset'] == ds]
            if not rf_row.empty and pd.notna(rf_row.iloc[0]['RF_Formula']):
                rf_f = str(rf_row.iloc[0]['RF_Formula'])
                if len(rf_f) > 100: rf_f = rf_f[:97] + "..."
                print(f"   [RF] (Tree #0): {rf_f}")
            else:
                print("   [RF] -")
        else:
            print("   [RF] -")

if __name__ == "__main__":
    final_df = load_results()
    
    if not final_df.empty:
        plot_comparison(final_df)
        print_summary(final_df)
    else:
        print("âŒ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")