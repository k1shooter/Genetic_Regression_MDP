import os
import sys
import glob
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tabulate import tabulate
from datetime import datetime

def find_latest_result(pattern):
    """ì£¼ì–´ì§„ íŒ¨í„´ì— ë§ëŠ” ê°€ì¥ ìµœì‹  CSV íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤."""
    files = glob.glob(pattern)
    if not files: return None
    return max(files, key=os.path.getctime)

def standardize_columns(df):
    """ë‹¤ì–‘í•œ ì´ë¦„ì˜ ì»¬ëŸ¼ì„ í‘œì¤€ ì´ë¦„ìœ¼ë¡œ ë³€ê²½í•©ë‹ˆë‹¤."""
    if df.empty: return df
    
    # ê³µë°± ì œê±°
    df.columns = df.columns.str.strip()
    
    rename_map = {
        # Accuracy
        'Accuracy': 'Acc', 'Test Acc': 'Acc', 'Test_Acc': 'Acc',
        'DNN_Acc': 'Acc', 'RF_Acc': 'Acc', 'DNN_Accuracy': 'Acc', 'RF_Accuracy': 'Acc',
        
        # F1 Score
        'F1_Score': 'F1', 'F1_Defective': 'F1', 'Test F1': 'F1', 'Test_F1': 'F1',
        'DNN_F1': 'F1', 'RF_F1': 'F1', 'DNN_F1_Score': 'F1', 'RF_F1_Score': 'F1',
        
        # MCC
        'Test MCC': 'MCC', 'Test_MCC': 'MCC', 
        'DNN_MCC': 'MCC', 'RF_MCC': 'MCC',
        'MCC Score': 'MCC',
        
        # Complexity
        'Complexity': 'Cplx',
        'Weighted_Cplx': 'W_Cplx'
    }
    
    # rename ì ìš©
    df = df.rename(columns=rename_map)
    
    # í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ 0.0ìœ¼ë¡œ ì´ˆê¸°í™”
    for col in ['Acc', 'F1', 'MCC']:
        if col not in df.columns:
            df[col] = 0.0
            
    # ë³µì¡ë„ ì»¬ëŸ¼ ì´ˆê¸°í™”
    for col in ['Cplx', 'W_Cplx']:
        if col not in df.columns:
            if col == 'W_Cplx' and 'Cplx' in df.columns:
                 df['W_Cplx'] = df['Cplx'] # W_Cplxê°€ ì—†ìœ¼ë©´ Cplx ë³µì‚¬
            else:
                 df[col] = 0.0
            
    return df

def load_chirps_formulas():
    """CHIRPS(Piecewise)ë¡œ ìƒì„±ëœ ìˆ˜ì‹ íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ë°ì´í„°ì…‹ë³„ë¡œ ë§¤í•‘í•©ë‹ˆë‹¤."""
    base_dir = "analysis_results/Piecewise"
    formula_map = {}
    
    if not os.path.exists(base_dir):
        return formula_map
        
    for dataset_name in os.listdir(base_dir):
        path = os.path.join(base_dir, dataset_name, "piecewise_formulas_metrics.csv")
        if os.path.exists(path):
            try:
                df = pd.read_csv(path)
                if not df.empty:
                    # ì¤‘ìš”ë„(Importance)ê°€ ê°€ì¥ ë†’ì€ Featureì˜ ìˆ˜ì‹ì„ ëŒ€í‘œê°’ìœ¼ë¡œ ì„ ì •
                    top_feature = df.sort_values(by='Importance', ascending=False).iloc[0]
                    formula_map[dataset_name] = f"[{top_feature['Feature']}] {top_feature['Formula']}"
            except Exception:
                pass
    return formula_map

def load_results():
    """ê° ëª¨ë¸ì˜ ìµœì‹  ê²°ê³¼ íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ í†µí•©í•©ë‹ˆë‹¤."""
    print("ğŸ“‚ ìµœì‹  ê²°ê³¼ íŒŒì¼ ë¡œë”© ì¤‘...")
    
    dfs = []

    # 1. DNN (Tuned)
    dnn_file = find_latest_result("optuna_dnn_results*.csv")
    if dnn_file:
        dnn_df = pd.read_csv(dnn_file)
        dnn_df = standardize_columns(dnn_df)
        dnn_df['Model'] = 'DNN (Tuned)'
        dnn_df['Strategy'] = '-' 
        dfs.append(dnn_df)
    else:
        print("âš ï¸ DNN ê²°ê³¼ íŒŒì¼ ì—†ìŒ")

    # 2. Random Forest (Tuned)
    rf_file = find_latest_result("optuna_rf_results*.csv")
    if rf_file:
        rf_df = pd.read_csv(rf_file)
        rf_df = standardize_columns(rf_df)
        rf_df['Model'] = 'RF (Tuned)'
        rf_df['Strategy'] = '-'
        dfs.append(rf_df)
    else:
        print("âš ï¸ RF ê²°ê³¼ íŒŒì¼ ì—†ìŒ")

    # 3. GP (GA-MO)
    gp_file = find_latest_result("ga_mo_results_*.csv")
    if gp_file:
        gp_raw = pd.read_csv(gp_file)
        gp_raw = standardize_columns(gp_raw)
        
        # Datasetë³„ MCCê°€ ê°€ì¥ ë†’ì€ ëª¨ë¸ í•˜ë‚˜ë§Œ ì„ íƒ
        if 'Target' in gp_raw.columns:
            mcc_target = gp_raw[gp_raw['Target'].str.upper() == 'MCC']
            if not mcc_target.empty:
                gp_best = mcc_target.sort_values(['Dataset', 'MCC'], ascending=[True, False]).drop_duplicates('Dataset')
            else:
                gp_best = gp_raw.sort_values(['Dataset', 'MCC'], ascending=[True, False]).drop_duplicates('Dataset')
        else:
            gp_best = gp_raw.sort_values(['Dataset', 'MCC'], ascending=[True, False]).drop_duplicates('Dataset')
            
        gp_df = gp_best.copy()
        gp_df['Model'] = 'GP (Ours)'
        
        if 'Strategy' not in gp_df.columns:
            gp_df['Strategy'] = 'Simple' 
            
        dfs.append(gp_df)
    else:
        print("âš ï¸ GP ê²°ê³¼ íŒŒì¼ ì—†ìŒ")

    if not dfs:
        return pd.DataFrame()

    # í†µí•©
    all_df = pd.concat(dfs, ignore_index=True)
    
    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ë‚¨ê¸°ê³ , NaNì€ 0ìœ¼ë¡œ ì±„ì›€
    cols = ['Dataset', 'Model', 'Strategy', 'Acc', 'F1', 'MCC', 'Cplx', 'W_Cplx', 'Formula']
    for col in cols:
        if col not in all_df.columns:
            all_df[col] = pd.NA
            
    num_cols = ['Acc', 'F1', 'MCC', 'Cplx', 'W_Cplx']
    all_df[num_cols] = all_df[num_cols].fillna(0.0)
    
    return all_df

def plot_comparison(df):
    """ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ ê·¸ë˜í”„"""
    if df.empty: return

    save_dir = "comparison_plots"
    os.makedirs(save_dir, exist_ok=True)
    
    metrics = ['F1', 'MCC']
    
    for metric in metrics:
        if metric not in df.columns: continue
        
        plt.figure(figsize=(12, 6))
        plot_data = df[df[metric] != 0] 
        
        if plot_data.empty: continue

        sns.barplot(data=plot_data, x='Dataset', y=metric, hue='Model', palette='viridis')
        plt.title(f'Model Comparison - {metric} Score', fontsize=15)
        plt.ylabel(metric)
        plt.ylim(-0.1, 1.1) 
        plt.grid(axis='y', linestyle='--', alpha=0.7)
        plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left')
        plt.tight_layout()
        
        filename = f"{save_dir}/comparison_{metric}.png"
        plt.savefig(filename)
        plt.close()
        print(f"ğŸ“Š {metric} ë¹„êµ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {filename}")

def print_performance_analysis(df):
    """[Part 1] ì„±ëŠ¥ ë¶„ì„ í…Œì´ë¸” ì¶œë ¥"""
    print("\n" + "="*80)
    print("ğŸ† Performance Analysis")
    print("="*80)

    cols = ['Dataset', 'Model', 'Acc', 'F1', 'MCC']

    # 1. MCC ê¸°ì¤€ ì •ë ¬ í…Œì´ë¸”
    print("\nğŸ“Œ Table 1: Ranked by MCC (Descending)")
    df_mcc = df.sort_values(by=['Dataset', 'MCC'], ascending=[True, False])
    print(tabulate(df_mcc[cols], headers=cols, tablefmt='simple', floatfmt=".4f"))

    # 2. F1 ê¸°ì¤€ ì •ë ¬ í…Œì´ë¸”
    print("\nğŸ“Œ Table 2: Ranked by F1 (Descending)")
    df_f1 = df.sort_values(by=['Dataset', 'F1'], ascending=[True, False])
    print(tabulate(df_f1[cols], headers=cols, tablefmt='simple', floatfmt=".4f"))

    # 3. Best Model ìš”ì•½ í…Œì´ë¸”
    print("\nğŸ“Œ Table 3: Best Models per Dataset (Performance)")
    
    summary_data = []
    datasets = sorted(df['Dataset'].unique())
    
    for ds in datasets:
        subset = df[df['Dataset'] == ds]
        if subset.empty: continue
        
        # Best MCC
        best_mcc_val = subset['MCC'].max()
        best_mcc_models = subset[subset['MCC'] == best_mcc_val]['Model'].tolist()
        best_mcc_str = ", ".join(best_mcc_models) + f" ({best_mcc_val:.3f})"
        
        # Best F1
        best_f1_val = subset['F1'].max()
        best_f1_models = subset[subset['F1'] == best_f1_val]['Model'].tolist()
        best_f1_str = ", ".join(best_f1_models) + f" ({best_f1_val:.3f})"
        
        summary_data.append([ds, best_f1_str, best_mcc_str])
        
    headers = ["Dataset", "Best Model (F1)", "Best Model (MCC)"]
    print(tabulate(summary_data, headers=headers, tablefmt="fancy_grid"))

def print_interpretability_analysis(df):
    """[Part 2] í•´ì„ ê°€ëŠ¥ì„± ë° ë³µì¡ë„ ë¶„ì„"""
    print("\n" + "="*80)
    print("ğŸ” Interpretability & Complexity Comparison")
    print("="*80)
    
    # --- [ì¶”ê°€] Best Complexity Model ê³„ì‚° (DNN ì œì™¸) ---
    df_comparable = df[~df['Model'].str.contains("DNN", na=False)].copy()
    grouped = df_comparable.groupby('Dataset')
    
    best_cplx_map = {}
    best_wcplx_map = {}
    
    for name, group in grouped:
        # Min Cplx
        min_cplx = group['Cplx'].min()
        winners_cplx = group[group['Cplx'] == min_cplx]['Model'].tolist()
        best_cplx_map[name] = ", ".join(winners_cplx)
        
        # Min W_Cplx
        min_wcplx = group['W_Cplx'].min()
        winners_wcplx = group[group['W_Cplx'] == min_wcplx]['Model'].tolist()
        best_wcplx_map[name] = ", ".join(winners_wcplx)

    # 1. ë³µì¡ë„ í…Œì´ë¸”
    print("\nğŸ“Œ Table 4: Complexity Metrics")
    cplx_data = []
    
    # Datasetë³„, ëª¨ë¸ë³„ ì •ë ¬
    df_sorted = df.sort_values(by=['Dataset', 'Model'])
    
    for _, row in df_sorted.iterrows():
        ds_name = row['Dataset']
        
        # DNNì€ ì œì™¸í•  ìˆ˜ë„ ìˆì§€ë§Œ, í‘œì—ëŠ” '-'ë¡œ í‘œì‹œí•´ì„œ ëª…ì‹œ
        if "DNN" in str(row['Model']):
            c_val, w_val = "-", "-"
        else:
            c_val = f"{float(row['Cplx']):.1f}"
            w_val = f"{float(row['W_Cplx']):.1f}"
            
        best_c = best_cplx_map.get(ds_name, "-")
        best_wc = best_wcplx_map.get(ds_name, "-")
        
        cplx_data.append([ds_name, row['Model'], c_val, w_val, best_c, best_wc])
        
    headers = ["Dataset", "Model", "Cplx", "W_Cplx", "Best (Cplx)", "Best (W_Cplx)"]
    print(tabulate(cplx_data, headers=headers, tablefmt="fancy_grid"))

    # 2. ìˆ˜ì‹ ë¹„êµ (Formula)
    print("\nğŸ“Œ Formula Comparison (GP vs RF)")
    
    gp_df = df[df['Model'] == 'GP (Ours)']
    rf_df = df[df['Model'] == 'RF (Tuned)']
    chirps_formulas = load_chirps_formulas()
    
    datasets = sorted(df['Dataset'].unique())
    
    for ds in datasets:
        print(f"\n Dataset: {ds}")
        
        # --- GP ì¶œë ¥ ---
        gp_row = gp_df[gp_df['Dataset'] == ds]
        if not gp_row.empty:
            cplx = gp_row.iloc[0]['Cplx']
            w_cplx = gp_row.iloc[0]['W_Cplx']
            form = gp_row.iloc[0]['Formula']
            if pd.isna(form): form = "-"
            
            print(f"   [GP] Complexity: Cplx:{cplx:.1f} | W_cplx:{w_cplx:.1f}")
            print(f"            Formula: {form}")
        else:
            print("   [GP] -")
            
        # --- RF (CHIRPS) ì¶œë ¥ ---
        rf_row = rf_df[rf_df['Dataset'] == ds]
        rf_cplx_str = "-"
        if not rf_row.empty:
            rf_cplx_str = f"{rf_row.iloc[0]['Cplx']:.1f}"
            
        rf_formula_str = "-"
        if ds in chirps_formulas:
            rf_formula_str = chirps_formulas[ds]
        else:
            rf_formula_str = "(No CHIRPS rule found)"
            
        print(f"   [RF] Complexity: Cplx:{rf_cplx_str}")
        print(f"            Formula: {rf_formula_str}")

if __name__ == "__main__":
    final_df = load_results()
    
    if not final_df.empty:
        plot_comparison(final_df)
        print_performance_analysis(final_df)
        print_interpretability_analysis(final_df)
        
        # ì „ì²´ ë°ì´í„° CSV ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"final_evaluation_summary_{timestamp}.csv"
        final_df.to_csv(filename, index=False)
        print(f"\nğŸ’¾ ì „ì²´ í†µí•© ê²°ê³¼ ì €ì¥: {filename}")
    else:
        print("âŒ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")