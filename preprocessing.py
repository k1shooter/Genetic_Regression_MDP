import os
import io
import numpy as np
import pandas as pd
import requests

from scipy.io import arff
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder

GITHUB_RAW_BASE = 'https://raw.githubusercontent.com/klainfo/NASADefectDataset/master/CleanedData/MDP/D\'\'/'
SAVE_DIR = './data'
DATASET_FILES = [
    'CM1.arff', 'JM1.arff', 'KC1.arff', 'KC3.arff', 
    'MC1.arff', 'MC2.arff', 'MW1.arff', 'PC1.arff', 'PC2.arff', 
    'PC3.arff', 'PC4.arff', 'PC5.arff'
]
FULL_PATHS = [GITHUB_RAW_BASE + f for f in DATASET_FILES]

def get_common_attributes(dataset_urls):
    """
    ì œê³µëœ ëª¨ë“  ARFF íŒŒì¼ì—ì„œ ê³µí†µìœ¼ë¡œ ì¡´ì¬í•˜ëŠ” ì†ì„± ëª©ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    (ê° íŒŒì¼ì˜ ë§ˆì§€ë§‰ ì—´, ì¦‰ íƒ€ê²Ÿ ë³€ìˆ˜ëŠ” ì œì™¸í•©ë‹ˆë‹¤.)
    """
    common_cols = None
    print("ğŸ“¡ ëª¨ë“  ë°ì´í„°ì…‹ì˜ ê³µí†µ ì†ì„± ëª©ë¡ ì¶”ì¶œ ì¤‘...")
    
    for full_url in dataset_urls:
        dataset_name = os.path.basename(full_url).replace('.arff', '')
        
        try:
            response = requests.get(full_url)
            response.raise_for_status() 

            content_string = response.content.decode('utf-8')
            data_io_string = io.StringIO(content_string)
            
            # arff íŒŒì‹± ì‹œ ë°ì´í„°í”„ë ˆì„ ìƒì„±ì„ ê±´ë„ˆë›°ê³  ë©”íƒ€ë°ì´í„°ë§Œ í™œìš©
            arff_data, meta = arff.loadarff(data_io_string)
            
            # ë§ˆì§€ë§‰ ì»¬ëŸ¼(íƒ€ê²Ÿ ë³€ìˆ˜)ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ì†ì„± ì´ë¦„ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
            current_cols = set(meta.names()[:-1])
            
            # ê³µí†µ ì†ì„± ì§‘í•© ì—…ë°ì´íŠ¸
            if common_cols is None:
                common_cols = current_cols
            else:
                common_cols = common_cols.intersection(current_cols)
                
            # KC4ì²˜ëŸ¼ ë°ì´í„°ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì²˜ë¦¬ (KC4ëŠ” ì†ì„± ì¶”ì¶œ ê°€ëŠ¥)
            if not common_cols:
                print(f"âš ï¸ ê²½ê³ : {dataset_name} ì²˜ë¦¬ í›„ ê³µí†µ ì†ì„± ì§‘í•©ì´ ë¹„ì—ˆìŠµë‹ˆë‹¤. (0ê°œ)")
                return []
                
        except Exception as e:
            # KC4ì²˜ëŸ¼ ë°ì´í„°ê°€ ë¹„ì–´ìˆê±°ë‚˜ (ë°ì´í„°ê°€ 0ê°œì´ì§€ë§Œ ARFF êµ¬ì¡°ëŠ” ì‚´ì•„ìˆëŠ” ê²½ìš°) 
            # íŒŒì‹±ì— ì‹¤íŒ¨í•˜ë©´ í•´ë‹¹ ë°ì´í„°ì…‹ì€ ê³µí†µ ì†ì„± ê³„ì‚°ì—ì„œ ì œì™¸ë˜ì–´ì•¼ í•˜ì§€ë§Œ,
            # ì—„ê²©í•˜ê²ŒëŠ” ëª¨ë“  ë°ì´í„°ì…‹ì— ìˆì–´ì•¼ í•˜ë¯€ë¡œ ì˜¤ë¥˜ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.
            print(f"âŒ ì˜¤ë¥˜: {dataset_name} ì†ì„± ì¶”ì¶œ ì‹¤íŒ¨ ({e}). ì´ ë°ì´í„°ì…‹ì„ í¬í•¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

    return sorted(list(common_cols))

def preprocess_and_save_data(full_url, save_directory, common_features):
    file_name = os.path.basename(full_url) 
    dataset_name = file_name.replace('.arff', '')

    try:
        response = requests.get(full_url)
        response.raise_for_status() 
        content_string = response.content.decode('utf-8')
        data_io_string = io.StringIO(content_string) 
        
        arff_data, meta = arff.loadarff(data_io_string)
        df = pd.DataFrame(arff_data)
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ ì˜¤ë¥˜: {dataset_name} íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨. URL: {full_url}, ì˜¤ë¥˜: {e}")
        return False
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {dataset_name} ARFF íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False
    
    try:
        if len(df) == 0:
            print(f"âš ï¸ ê²½ê³ : {dataset_name} ë°ì´í„°ì…‹ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            return False

        target_col = None
        for name in df.columns:
            if name in ['Defective', 'label']:
                target_col = name
                break
        if target_col is None:
             target_col = df.columns[-1]


        df[target_col] = df[target_col].apply(lambda x: x.decode('utf-8'))
        le = LabelEncoder()
        y = pd.Series(le.fit_transform(df[target_col]), name='Defective_Encoded')
        X_all = df.drop(columns=[target_col]).copy()
        X = X_all[[col for col in common_features if col in X_all.columns]].copy()
        if 'Defective_Encoded' in X.columns:
            X = X.drop(columns=['Defective_Encoded'])

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, stratify=y
        )

        # ìŠ¤ì¼€ì¼ë§ dnnìš©
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)
        X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)

    except Exception as e:
        print(f"ì˜¤ë¥˜: {dataset_name} ë¡œë“œ ë° ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False
        
    if not os.path.exists(save_directory):
        os.makedirs(save_directory)

    try:
        pd.concat([X_train.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1).to_csv(os.path.join(save_directory, f'{dataset_name}_train_rf.csv'), index=False)
        pd.concat([X_test.reset_index(drop=True), y_test.reset_index(drop=True)], axis=1).to_csv(os.path.join(save_directory, f'{dataset_name}_test_rf.csv'), index=False)
        
        pd.concat([X_train_scaled_df.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1).to_csv(os.path.join(save_directory, f'{dataset_name}_train_pt.csv'), index=False)
        pd.concat([X_test_scaled_df.reset_index(drop=True), y_test.reset_index(drop=True)], axis=1).to_csv(os.path.join(save_directory, f'{dataset_name}_test_pt.csv'), index=False)

        print(f"âœ… {dataset_name} ì „ì²˜ë¦¬ ë° ì €ì¥ ì™„ë£Œ.")
        return True

    except Exception as e:
        print(f"ì˜¤ë¥˜: {file_name} ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

def run_preprocessing_pipeline(full_paths, save_directory):
    print(f"ì „ì²˜ë¦¬ ë°ì´í„°ë¥¼ {save_directory}ì— ì €ì¥í•©ë‹ˆë‹¤.")

    print("ê³µí†µì†ì„± ì¶”ì¶œ")
    commons = get_common_attributes(FULL_PATHS)
    print("ë")
    for path in full_paths:
        preprocess_and_save_data(path, save_directory, commons)

run_preprocessing_pipeline(FULL_PATHS, SAVE_DIR)