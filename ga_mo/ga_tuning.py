import optuna
import os
import sys
import numpy as np
import pandas as pd
from tqdm import tqdm
from sklearn.metrics import f1_score, matthews_corrcoef
from sklearn.model_selection import StratifiedKFold

# [ìˆ˜ì • 1] ìˆœìˆ˜ GA(evolution) ëŒ€ì‹  RL-GEP(rl_gep)ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
from rl_gep import MultiObjectiveGP
from util import load_data_robust

DATASET_NAMES = ['CM1', 'JM1', 'KC1', 'KC3', 'MC1', 'MC2', 'MW1', 'PC1', 'PC2', 'PC3', 'PC4', 'PC5']

# Optuna ìžì²´ ë¡œê·¸ëŠ” ì¤„ì´ê³ , tqdmìœ¼ë¡œ ì§„í–‰ ìƒí™©ì„ í™•ì¸í•©ë‹ˆë‹¤.
optuna.logging.set_verbosity(optuna.logging.WARNING)

def load_data(dataset_name):
    # ì „ì²˜ë¦¬ëœ RFìš© ë°ì´í„° ë¡œë“œ
    return load_data_robust(dataset_name, data_type='rf')

def objective(trial, dataset_name, X_full, y_full, target_metric='mcc'):
    """
    [Fast Mode] 3-Fold CV í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (RL-GEP ì ìš©)
    """
    # 1. íƒìƒ‰ ê³µê°„ ì •ì˜ (Fast Mode: ì¸êµ¬ìˆ˜ì™€ ì„¸ëŒ€ìˆ˜ë¥¼ ì¤„ìž„)
    pop_size = trial.suggest_categorical('pop_size', [100, 150]) 
    generations = 30 # íŠœë‹ ì†ë„ë¥¼ ìœ„í•´ 30ì„¸ëŒ€ë¡œ ì œí•œ
    
    max_depth = trial.suggest_int('max_depth', 4, 7)
    crossover_rate = trial.suggest_float('crossover_rate', 0.7, 0.95)
    mutation_rate = trial.suggest_float('mutation_rate', 0.1, 0.4)
    
    # [ìˆ˜ì • 2] RL ê´€ë ¨ íŒŒë¼ë¯¸í„° ì¶”ê°€ (RL-GEP í™œì„±í™”)
    rl_hybrid_ratio = trial.suggest_float('rl_hybrid_ratio', 0.1, 0.6)
    rl_learning_rate = trial.suggest_float('rl_learning_rate', 0.001, 0.01)
    
    complexity_strategy = 'simple' # íŠœë‹ ë³µìž¡ë„ ìµœì†Œí™”

    # 2. Stratified K-Fold ì„¤ì • (3-Fold)
    n_splits = 3
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    
    fold_scores = []
    
    # 3. êµì°¨ ê²€ì¦ ìˆ˜í–‰
    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_full, y_full)):
        X_train, X_val = X_full[train_idx], X_full[val_idx]
        y_train, y_val = y_full[train_idx], y_full[val_idx]
        
        # [ìˆ˜ì • 3] ì§„í–‰ë°”(tqdm) ì„¤ëª… ë¬¸êµ¬ êµ¬ì²´í™”
        # ì˜ˆ: [CM1] T0-F1 (Trial 0, Fold 1)
        desc = f"[{dataset_name}] T{trial.number}-F{fold_idx+1}"
        
        # ëª¨ë¸ ì´ˆê¸°í™” (RL-GEP)
        moga = MultiObjectiveGP(
            n_features=X_train.shape[1],
            pop_size=pop_size,
            generations=generations,
            max_depth=max_depth,
            crossover_rate=crossover_rate,
            mutation_rate=mutation_rate,
            metric=target_metric,
            complexity_strategy=complexity_strategy,
            # RL íŒŒë¼ë¯¸í„° ì „ë‹¬
            rl_hybrid_ratio=rl_hybrid_ratio,
            rl_learning_rate=rl_learning_rate,
            random_state=42 + fold_idx, 
            description=desc # ë¡œê·¸ ì „ë‹¬
        )
        
        # í•™ìŠµ (Pareto Front ë°˜í™˜)
        pareto_front = moga.fit(X_train, y_train)
        
        # ê²€ì¦ (Best Threshold ì ìš©)
        best_fold_score = -1.0
        
        for ind in pareto_front:
            thresh = getattr(ind, 'best_threshold', 0.5)
            
            try:
                logits = np.clip(ind.evaluate(X_val), -20, 20)
                probs = 1 / (1 + np.exp(-logits))
                preds = (probs >= thresh).astype(int)
                
                if target_metric == 'f1':
                    score = f1_score(y_val, preds, pos_label=1, zero_division=0)
                else:
                    score = matthews_corrcoef(y_val, preds)
            except:
                score = 0.0
                
            if score > best_fold_score:
                best_fold_score = score
        
        if best_fold_score < 0:
            best_fold_score = 0.0
            
        fold_scores.append(best_fold_score)

    # 4. K-Fold í‰ê·  ì ìˆ˜ ë°˜í™˜
    return np.mean(fold_scores)

def tune_ga(target_metric='mcc'):
    print(f"\nâš¡ [FAST MODE] Tuning RL-GEP (Target: {target_metric.upper()})...")
    results = []
    
    for name in DATASET_NAMES:
        X_train_df, y_train_df, _, _ = load_data(name)
        
        if X_train_df is None: 
            print(f"   âš ï¸ Skipping {name} (Data not found)")
            continue
            
        # Numpy ë³€í™˜ (í•„ìˆ˜)
        X_train = X_train_df.values
        y_train = y_train_df.values
        
        print(f"   ðŸ‘‰ Processing {name} (3-Fold CV)...")
        
        study = optuna.create_study(direction='maximize')
        
        # Trial íšŸìˆ˜ (ì‹œê°„ì— ë”°ë¼ ì¡°ì ˆ, Fast Mode = 5~10íšŒ)
        n_trials = 5
        study.optimize(lambda t: objective(t, name, X_train, y_train, target_metric), n_trials=n_trials)
        
        best_params = study.best_params
        best_val = study.best_value
        
        print(f"      âœ… Best CV {target_metric.upper()}: {best_val:.4f}")
        
        results.append({
            'Dataset': name,
            'Metric': target_metric.upper(),
            'Best_Params': best_params,
            'Best_CV_Score': best_val
        })
        
    # ê²°ê³¼ ì €ìž¥
    filename = f"ga_tuning_{target_metric}_results.csv"
    pd.DataFrame(results).to_csv(filename, index=False)
    print(f"\nðŸ’¾ {target_metric.upper()} Tuning Results saved to '{filename}'")

if __name__ == "__main__":
    # MCC ê¸°ì¤€ íŠœë‹
    tune_ga('mcc')
    
    # F1 ê¸°ì¤€ íŠœë‹ (í•„ìš” ì‹œ ì£¼ì„ í•´ì œ)
    tune_ga('f1')