import os
import sys
import numpy as np
import pandas as pd
from tabulate import tabulate
from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef
from datetime import datetime
##############ê¹€ìŠ¹ì¤€ : ìˆ˜ì •, Seed ë°›ìœ¼ë©´ ë°˜ì˜, ì•ˆë°›ìœ¼ë©´ ì›ë˜ëŒ€ë¡œ
from sklearn.ensemble import RandomForestClassifier
from gptree import Node, FUNCTIONS
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)
try:
    from classifiers.chirps_full import CHIRPSExplainerEnhanced
except ImportError:
    print("Warning: Could not import CHIRPSExplainerEnhanced. Check directory structure.")
    CHIRPSExplainerEnhanced = None
##############
sys.path.append(os.path.abspath(os.path.dirname(__file__)))
from evolution import MultiObjectiveGP
from util import load_data_robust

DATASET_NAMES = ['CM1', 'JM1', 'KC1', 'KC3', 'MC1', 'MC2', 'MW1', 'PC1', 'PC2', 'PC3', 'PC4', 'PC5']
#DATASET_NAMES = ['CM1', 'JM1', 'KC1']


######################################## ê¹€ìŠ¹ì¤€ :seedingìš© í•¨ìˆ˜ë“¤
# [ìƒˆë¡œ ì¶”ê°€ëœ í•¨ìˆ˜] CHIRPS Ruleì„ GAìš© ì‚°ìˆ  íŠ¸ë¦¬ë¡œ ë³€í™˜
def convert_rule_to_arithmetic_tree(rule, scaling_factor=10.0):
    """
    CHIRPS Ruleì„ GAìš© ì‚°ìˆ  íŠ¸ë¦¬ë¡œ ë³€í™˜ (Stronger Version)
    1. ì¡°ê±´ ê²°í•©: ë§ì…ˆ(+) ëŒ€ì‹  ê³±ì…ˆ(*)ì„ ì‚¬ìš©í•´ ì¡°ê±´ ê°„ ìƒí˜¸ì‘ìš© ê°•í™” (Option)
       -> ë‹¤ë§Œ, ìŒìˆ˜*ìŒìˆ˜=ì–‘ìˆ˜ ë¬¸ì œê°€ ìƒê¸¸ ìˆ˜ ìˆì–´ ì•ˆì „í•˜ê²Œ ë§ì…ˆ ìœ ì§€í•˜ë˜ ì¦í­í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì„.
       -> ì—¬ê¸°ì„œëŠ” ê°€ì¥ ì•ˆì „í•˜ê³  ê°•ë ¥í•œ 'ë§ì…ˆ ê²°í•© í›„ ì¦í­' ë°©ì‹ì„ ì ìš©í•©ë‹ˆë‹¤.
    2. ì‹ í˜¸ ì¦í­: ìµœì¢… ê²°ê³¼ì— scaling_factorë¥¼ ê³±í•´ 0.5 ê·¼ì²˜ê°€ ì•„ë‹Œ 0 or 1ì— ê°€ê¹Œìš´ í™•ë¥  ë°°ì¶œ
    """
    if not rule:
        return None

    f_add = FUNCTIONS['add'][0]
    f_sub = FUNCTIONS['sub'][0]
    f_mul = FUNCTIONS['mul'][0] # [New] ê³±ì…ˆ í•¨ìˆ˜

    nodes = []
    for feat_idx, op, thresh in rule:
        feat_node = Node(val=feat_idx) 
        const_node = Node(val=float(thresh))
        
        # [Step 1] ê¸°ë³¸ ì‚°ìˆ ì‹ ìƒì„± (Gradient ìœ ì§€)
        if op == '<=':
            # (Threshold - Feature) : ì¡°ê±´ ë§Œì¡± ì‹œ ì–‘ìˆ˜
            term = Node(None, func=f_sub, children=[const_node, feat_node])
        else: # '>'
            # (Feature - Threshold) : ì¡°ê±´ ë§Œì¡± ì‹œ ì–‘ìˆ˜
            term = Node(None, func=f_sub, children=[feat_node, const_node])
            
        nodes.append(term)
    
    if not nodes:
        return None
        
    # [Step 2] ì¡°ê±´ ê²°í•© (ì—¬ê¸°ì„œëŠ” ì•ˆì „ì„±ì„ ìœ„í•´ Add ìœ ì§€)
    # ê³±ì…ˆ(Mul)ì„ ì“°ë©´ (-1) * (-1) = +1 ì´ ë˜ì–´, ë‘˜ ë‹¤ í‹€ë ¸ëŠ”ë° ë§ì•˜ë‹¤ê³  ì°©ê°í•  ìœ„í—˜ì´ í¼
    combined_tree = nodes[0]
    for i in range(1, len(nodes)):
        combined_tree = Node(None, func=f_add, children=[combined_tree, nodes[i]])

    # [Step 3] ğŸ”¥ ê°•ë ¥í•œ ì‹ í˜¸ ì¦í­ (Scaling)
    # ì „ì²´ ì‹ì— 10.0 ë“±ì„ ê³±í•´ì„œ, ì¡°ê¸ˆë§Œ ë§Œì¡±í•´ë„ Sigmoid í™•ë¥ ì´ 1.0ì— ê°€ê¹ê²Œ ì°íˆë„ë¡ í•¨
    # ìˆ˜ì‹: (Cond1 + Cond2) * 10.0
    strong_seed = Node(None, func=f_mul, children=[
        combined_tree,
        Node(val=scaling_factor) # ê°•ë ¥í•œ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ 10.0)
    ])
        
    return strong_seed

# [ìƒˆë¡œ ì¶”ê°€ëœ í•¨ìˆ˜] CHIRPS ì‹¤í–‰ ë° Seed ìƒì„±
def get_chirps_seeds(X_train, y_train, n_seeds=20):
    if CHIRPSExplainerEnhanced is None:
        return []

    print("ğŸŒ² Generating seeds via CHIRPS...")
    
    # DataFrame ë³€í™˜ (CHIRPS í˜¸í™˜ì„±)
    if isinstance(X_train, np.ndarray):
        df_X = pd.DataFrame(X_train, columns=[f"x{i}" for i in range(X_train.shape[1])])
    else:
        df_X = X_train.copy()
    
    if isinstance(y_train, np.ndarray):
        s_y = pd.Series(y_train)
    else:
        s_y = y_train.copy()

    # ê°€ë²¼ìš´ RF ëª¨ë¸ í•™ìŠµ
    rf = RandomForestClassifier(n_estimators=30, max_depth=5, random_state=42, n_jobs=-1)
    rf.fit(df_X, s_y)
    
    num_classes = len(np.unique(s_y))
    explainer = CHIRPSExplainerEnhanced(rf, df_X, s_y, num_classes)
    
    # Defective(1) ìƒ˜í”Œ ì¤‘ ì¼ë¶€ ìƒ˜í”Œë§
    target_indices = np.where(s_y == 1)[0]
    if len(target_indices) > n_seeds:
        np.random.shuffle(target_indices)
        target_indices = target_indices[:n_seeds]
    
    seeds = []
    seen_rules = set()
    
    for idx in target_indices:
        instance = df_X.iloc[idx]
        try:
            exp = explainer.explain_instance(instance)
            if exp and exp['rule']:
                rule_str = str(exp['rule'])
                if rule_str not in seen_rules:
                    seen_rules.add(rule_str)
                    # íŠ¸ë¦¬ ë³€í™˜
                    tree_seed = convert_rule_to_arithmetic_tree(exp['rule'])
                    if tree_seed:
                        seeds.append(tree_seed)
        except Exception:
            continue
            
    print(f"âœ¨ Extracted {len(seeds)} CHIRPS seeds.")
    return seeds
#########################################
def optimize_and_evaluate(dataset_name, X_train, y_train, X_test, y_test, target_metric, seeds=None):
    """ì§€ì •ëœ metricìœ¼ë¡œ ìµœì í™” ë° í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜"""
    print(f"   ğŸ‘‰ Optimizing Target: {target_metric.upper()}")
    
    # evolution.pyê°€ metric ì¸ìë¥¼ ë°›ë„ë¡ ìˆ˜ì •ë˜ì—ˆë‹¤ê³  ê°€ì •
    moga = MultiObjectiveGP(
        n_features=X_train.shape[1], pop_size=300, generations=100, max_depth=6,
        crossover_rate=0.9, mutation_rate=0.1, random_state=42, metric=target_metric
    )
    pareto_front = moga.fit(X_train, y_train, seeds=seeds)
    
    results = []
    unique_solutions = {}
    
    for ind in pareto_front:
        logits = np.clip(ind.evaluate(X_test), -20, 20)
        preds = np.round(1 / (1 + np.exp(-logits)))
        
        # ì§€í‘œ ê³„ì‚°
        acc = accuracy_score(y_test, preds)
        f1 = f1_score(y_test, preds, pos_label=1, zero_division=0)
        mcc = matthews_corrcoef(y_test, preds)
        
        formula = str(ind)
        if formula not in unique_solutions:
            unique_solutions[formula] = {
                'Dataset': dataset_name,
                'Target': target_metric.upper(),
                'Train_F1': ind.f1_score,
                'Train_MCC': ind.mcc_score,
                'Test_Acc': acc,
                'Test_F1': f1,
                'Test_MCC': mcc,
                'Complexity': ind.size(),
                'Formula': formula
            }
    return list(unique_solutions.values())

def run_mo_ga_on_dataset(dataset_name, need_seed = False):
    print(f"\nğŸš€ {dataset_name} Multi-Objective ë¶„ì„ ì‹œì‘...")
    # ë…ë¦½ ì „ì²˜ë¦¬ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ util.py ìˆ˜ì • í˜¹ì€ ì§ì ‘ ë¡œë“œ í•„ìš” (í˜„ì¬ëŠ” ê¸°ì¡´ ìœ ì§€)
    X_train, y_train, X_test, y_test = load_data_robust(dataset_name, data_type='pt')
    
    if X_train is None: return []
    if need_seed:
        seeds = get_chirps_seeds(X_train, y_train, n_seeds=20)
    else:
        seeds = None

    data = (X_train.values, y_train.values, X_test.values, y_test.values)
    dataset_results = []
    
    # ë‘ ê°€ì§€ ëª©í‘œë¡œ ê°ê° ìµœì í™” ìˆ˜í–‰
    for target in ['f1', 'mcc']:
        dataset_results.extend(optimize_and_evaluate(dataset_name, *data, target, seeds=seeds))
    
    # ì •ë ¬: Target -> Complexity -> F1 (ë‚´ë¦¼ì°¨ìˆœ)
    dataset_results.sort(key=lambda x: (x['Target'], x['Complexity'], -x['Test_F1']))
    print(f"âœ… {dataset_name} ì™„ë£Œ. ì´ Solution ìˆ˜: {len(dataset_results)}")
    
    return dataset_results

if __name__ == "__main__":
    print("="*60 + "\nğŸ§¬ Dual-Objective GA (F1 & MCC) for Defect Prediction\n" + "="*60)
    
    all_results = []
    for name in DATASET_NAMES:
        all_results.extend(run_mo_ga_on_dataset(name, need_seed = True))
            
    if all_results:
        headers = ["Dataset", "Target", "Cplx", "F1", "MCC", "Acc", "Formula"]
        table_data = []
        for r in all_results:
            fmt_form = r['Formula'] if len(r['Formula']) < 50 else r['Formula'][:47] + "..."
            table_data.append([
                r['Dataset'], r['Target'], r['Complexity'], 
                f"{r['Test_F1']:.4f}", f"{r['Test_MCC']:.4f}", f"{r['Test_Acc']:.4f}", fmt_form
            ])
            
        print("\n" + tabulate(table_data, headers=headers, tablefmt="simple"))
        
        filename = f'ga_mo_results_{datetime.now().strftime("%m%d_%H%M%S")}.csv'
        pd.DataFrame(all_results).to_csv(filename, index=False)
        print(f"\nğŸ’¾ ê²°ê³¼ê°€ '{filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")